# Ocean-Powered Image Processor for Social Media Content

## Project Overview

This project explores the use of **Ocean Protocol** for offloading the computational tasks involved in image processing, specifically for creating engaging visual content suitable for platforms like Instagram. The core idea is to build an image processing algorithm (using Node.js and Sharp) that can be published and run within Ocean Protocol's Compute-to-Data (C2D) environment.

This initiative draws inspiration from a prior Python-based project designed to:
1.  Generate detailed and engaging text content using an LLM (specifically, **Qwen models accessed via Alibaba Cloud's DashScope, using an OpenAI-compatible API**).
2.  Overlay this generated text onto images or create visually appealing slides.
3.  Prepare these images for easy sharing on social media platforms such as Instagram.

## Goal of This Node.js & Ocean Protocol Project

While the Python project handled both text generation and image creation, **this Node.js project focuses on the image manipulation aspect (applying filters, adding text overlays) and making this processing power available via Ocean Protocol.**

The primary goals are:
* To create a reusable image processing algorithm in Node.js.
* To enable this algorithm to be run as a compute job on Ocean Protocol, allowing data owners to process their images without directly sharing the raw image files.
* To demonstrate how inputs (like the image itself, filter types, and text for overlays) can be securely provided to an Ocean C2D job.

Essentially, the vision is to decouple the text generation (which could still come from your Qwen-powered Python scripts or any other source) from the image processing. The detailed text generated by Qwen could become an *input* to this Ocean-powered image processing algorithm.

## Key Features (Node.js Image Processor)

* **Image Input Flexibility**: Designed to accept images via:
    * Direct file paths (typical in Ocean C2D where data is mounted).
    * Ocean Protocol DIDs (Decentralized Identifiers), allowing the algorithm to fetch data assets published on Ocean.
    * Direct URLs.
* **Image Filtering**: Applies common image filters like grayscale, blur, unsharp mask, etc., using the Sharp library.
* **Text Overlay**: Adds customizable text overlays onto images, with options for color and basic styling.
* **Ocean C2D Compatibility**:
    * Configurable via environment variables (e.g., `INPUT_DID_0`, `INPUT_FILE_PATH_0`, `INPUT_FILTERTYPE`, `INPUT_TEXT`).
    * Writes processed outputs to standard C2D output directories (e.g., `/data/outputs`), including a `results.json` metadata file.
    * Includes `ocean.js` for interacting with the Ocean Protocol network (e.g., resolving DIDs).
* **Server Component (`server.js`)**: A basic Express.js server is also included to demonstrate direct API interaction for image processing, separate from the C2D flow.

## How This Differs from the Inspiriting Python Project

* **Compute Environment**: This project leverages **Ocean Protocol for compute resource provisioning (CPU)**, allowing for decentralized and privacy-preserving image processing. The Python project ran these tasks locally or on a traditional server.
* **Image Processing Stack**: This project uses **Node.js and the Sharp library** for image manipulation, while the Python project used Pillow.
* **Focus**: This project isolates the *image processing* component to be an Ocean-compatible algorithm. The text generation logic (using Qwen) from the Python project would be an *upstream process* providing input to this one.

## Future Vision & Integration

The detailed text generated by your Qwen API (as seen in the Python `image_generator.py` using `qwen-plus` via DashScope) can serve as a dynamic input for the `INPUT_TEXT` parameter of the `image-processing.js` script in this project.

**Workflow Envisioned:**

1.  **Text Generation (External/Python Project)**: Use your existing Qwen-powered scripts to generate compelling captions, quotes, or slide content.
2.  **Data Asset Publication (Optional)**: The base images and/or the generated text could be published as Ocean data assets.
3.  **Compute Job on Ocean Protocol**:
    * Users select this Node.js image processing algorithm on the Ocean Market.
    * They provide their image (as a DID, URL, or direct upload if the C2D environment supports it) and the Qwen-generated text as parameters.
    * The Ocean C2D environment executes `image-processing.js` with these inputs.
4.  **Output**: The final, processed image (with filters and the detailed Qwen text overlay) is produced and made available to the user, ready for platforms like Instagram.

This approach allows for a modular system where specialized AI (like Qwen for text) and specialized compute (like Ocean for image processing) can be combined effectively.

## Running This Node.js Project

(Refer to the instructions provided previously for setting up `image-processing.js` with environment variables for local testing or deployment as an Ocean C2D algorithm.)

---

This README should provide a good context for anyone looking at your Node.js project, explaining its purpose and how it relates to your broader goals and previous work.